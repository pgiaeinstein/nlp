{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Atividade 3\n",
    "\n",
    "## Redes Neurais Recorrentes\n",
    "\n",
    "Discutimos como uma rede neural recorrente, através de suas células de memória, permitem processamento de dados sequencial. Também falamos sobre embedding e fizemos um exercício para entender o objetivo e função de um Word2Vec.\n",
    "\n",
    "A biblioteca Keras traz acesso alto nível a camadas que permitem a fácil implementação deste tipo de layer.\n",
    "\n",
    "Este tipo de técnica é boa para generalizar informação esparsa condensando em uma camada densa:\n",
    "\n",
    "![word embeddings vs. one hot encoding](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)\n",
    "\n",
    "Vamos ver exemplos de implementação destas camadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM, CuDNNLSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacks de RNNs\n",
    "\n",
    "Em alguns casos, é interessante criar uma sequência de camadas recorrentes para processamento da informação. Nestes casos é necessário retornar as sequências para compartilhar com suas camadas vizinhas, com exceção da camada final. Este processo é feito passando o valor booleano `True` no o parâmetro `return_sequences` destas camadas, veja exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))  # Este layer final apenas retorna os últimos outputs.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise e classificação de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "Nesta atividade utilizaremos um _dataset_ chamado __IMDB__ composto de opiniões sobre filmes em formato textual. O objetivo é classificar estes _inputs_ de forma binária (0 e 1) onde identificaremos se a opinião é _**positiva**_ ou _**negativa**_.\n",
    "\n",
    "__IMDB__ (Internet Movie DataBase) conta com um total de 50.000 opiniões sobre filmes onde separamos 25.000 delas para treino e os 50% restantes para teste. As frações são balanceadas, ou seja, contem um número igual de opiniões positivas e negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000 # Número máximo de palavras consideradas como features\n",
    "max_len      = 500 # Tamanho máximo do texto utilizado como input\n",
    "batch_size   = 32 # Tamanho do batch de processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "print(len(x_train), 'sequências de treino')\n",
    "print(len(x_test), 'sequências de teste')\n",
    "\n",
    "print('PadSequences (amostras x tamanho)')\n",
    "input_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "input_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PadSequence\n",
    "\n",
    "Precisamos formatar um padrão de input para nossa rede. Definimos que nosso input máximo são 500 palavras. O que este processo faz é cortar textos maiores e preencher com 0 o que falta para completar 500 posições em textos menores que 500 palavras em seu conteúdo.\n",
    "\n",
    "Veja exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train[0]))\n",
    "x_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ter ideia do conteúdo deste dado, é possível baixar o dicionário representativo da informação. Da mesma maneira que fizemos na aula passada, podemos consultar o valor de cada número neste dicionário.\n",
    "\n",
    "Veja o exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_palavras = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_palavras['woody']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dicionário possui o valor da palavra como chave, podemos inverter seu conteúdo de `palavra : número` para `número : palavra` para facilitar nosso processamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_palavras_formatado = dict([(valor, chave) for (chave, valor) in dic_palavras.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_palavras_formatado[2289]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos iterar sobre o vetor de _input_ traduzindo a informação, vejamos o exemplo na posição `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = []\n",
    "\n",
    "for item in x_train[0]:\n",
    "    texto.append(dic_palavras_formatado.get(item, '?'))\n",
    "    \n",
    "print(texto)\n",
    "print('------------------*------------------')\n",
    "print(' '.join(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando a rede\n",
    "\n",
    "Podemos treinar uma rede de arquitetura simples para resolver o problema acima utilizando as camadas de `Embedding` e `SimpleRNN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss, acc] = model.evaluate(input_test, y_test)\n",
    "print(\"Acc: {:.4f}\".format(acc))\n",
    "\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label='Acc no treino')\n",
    "plt.plot(epochs, val_acc, label='Acc da validação')\n",
    "plt.title('Acc no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, label='Loss no treino')\n",
    "plt.plot(epochs, val_loss, label='Loss na validação')\n",
    "plt.title('Loss no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(input_test).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_preds)\n",
    "auc_calc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(fpr, tpr, label='Modelo (area = {:.3f})'.format(auc_calc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='best')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Vamos para um exemplo mais objetivo, vamos implementar a mesma arquitetura utilizando uma camada de **LSTM**.\n",
    "\n",
    "Keras traz uma implementação para processamento em GPUs com arquitetura CUDA. Se em seu ambiente, você possui uma GPU com este tipo de arquitetura, pode utilizar a classe `CuDNNLSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss, acc] = model.evaluate(input_test, y_test)\n",
    "print(\"Acc: {:.4f}\".format(acc))\n",
    "\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label='Acc no treino')\n",
    "plt.plot(epochs, val_acc, label='Acc da validação')\n",
    "plt.title('Acc no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, label='Loss no treino')\n",
    "plt.plot(epochs, val_loss, label='Loss na validação')\n",
    "plt.title('Loss no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(input_test).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_preds)\n",
    "auc_calc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(fpr, tpr, label='Modelo (area = {:.3f})'.format(auc_calc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='best')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício Proposto\n",
    "\n",
    "Explore outras arquiteturas de rede e veja se você consegue obter melhores resultados com o exemplo visto acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, 32))\n",
    "model2.add(LSTM(32, return_sequences=True))\n",
    "model2.add(LSTM(32, return_sequences=True))\n",
    "model2.add(LSTM(32))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model2.fit(input_train, y_train, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss, acc] = model.evaluate(input_test, y_test)\n",
    "print(\"Acc: {:.4f}\".format(acc))\n",
    "\n",
    "acc      = history.history['acc']\n",
    "val_acc  = history.history['val_acc']\n",
    "loss     = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, label='Acc no treino')\n",
    "plt.plot(epochs, val_acc, label='Acc da validação')\n",
    "plt.title('Acc no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, label='Loss no treino')\n",
    "plt.plot(epochs, val_loss, label='Loss na validação')\n",
    "plt.title('Loss no treino e validação')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(input_test).flatten()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_preds)\n",
    "auc_calc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize = (15, 10))\n",
    "plt.plot(fpr, tpr, label='Modelo (area = {:.3f})'.format(auc_calc))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='best')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura\n",
    "\n",
    "### Um problema de previsão\n",
    "\n",
    "Problemas envolvendo sequências podem ser encontrados em outros formatos. No meio médico; Sequências de imagens em um exame de tomografia ou valores de oxigenação de um paciente monitorado, são exemplos.\n",
    "\n",
    "Pensando em negócio, um modelo capaz de prever temperatura em um certo período pode ser utilizado como feature em um motor de tomada de decisão. Dentro do meio hospitalar, pode ser utilizado como feature para prever a demanda diária de uma unidade de pronto atendimento.\n",
    "\n",
    "O artigo em anexo abaixo faz parte do livro [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff) e traz um exemplo de previsão utilizando um conjunto de informações coletadas na estação meteorológica do [Instituto Max-Plank para biogeoquímica em Jena na Alemanha](http://www.bgc-jena.mpg.de/wetter/).\n",
    "\n",
    "Além de trazer o uso de LSTMs bidirecionais, o que veremos na próxima aula deste curso, mostra um exemplo de consumo sequencial de informação pela rede.\n",
    "\n",
    "[Link para download da base](https://www.kaggle.com/stytch16/jena-climate-2009-2016)\n",
    "[Link para o artigo](https://github.com/pgiaeinstein/nlp/blob/master/fcholletRNNS.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('https://raw.githubusercontent.com/pgiaeinstein/nlp/master/exemplo_prox_aula.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas futuros\n",
    "\n",
    "Em nossa próxima aula, estudaremos a utilização das ConvNets nas camadas de uma rede neural. Trabalharemos com imagens e novamente com texto. O objetivo é modelar uma rede capaz de extrair entidades dos textos que processamos nas atividades anteriores desta aula.\n",
    "\n",
    "Para exemplificar, veja a tabela abaixo. Vimos uma forma de encontrar padrões textuais utilizando expressões regulares, o que nos possibilita segmentar o texto classificando cada palavra.\n",
    "\n",
    "Vamos ver uma abordagem de resolver o problema através de uma rede neural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmPNRv9OWT7m"
   },
   "source": [
    "\n",
    "# Lab 01\n",
    "\n",
    "## Introdução\n",
    "\n",
    "O objetivo deste laboratório é explorar diferentes técnicas utilizadas para classificação em aprendizado de máquina, utilizaremos modelos clássicos de classificação e introduziremos um modelo simples de rede neural para resolver o mesmo problema.\n",
    "\n",
    "O intuito desta atividade é a familiarização das bibliotecas em Python utilizadas por padrão na análise de dados assim como mostrar que o aprendizado de máquina é algo simples e acessível a qualquer um.\n",
    "\n",
    "## Sobre o DataSet\n",
    "\n",
    "Para este laboratório, vamos brincar com o dataset [Breast Cancer Wisconsin](http://mlr.cs.umass.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "\n",
    "Este dataset é público e foi disponibilizado em novembro de 1995, o objetivo é classificar tumores como benignos ou malignos considerando valores obtidos por análise de imagem.\n",
    "\n",
    "Mais informações sobre este dataset e seus valores podem ser consultadas [neste link](http://mlr.cs.umass.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names).\n",
    "\n",
    "## Bibliotecas utilizadas\n",
    "\n",
    "### NumPy\n",
    "\n",
    "[NumPy](http://www.numpy.org/) é uma famosa biblioteca utilizada para fins científicos, facilita a criação, manipulação e cálculos envolvendo vetores e matrizes.\n",
    "\n",
    "### Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) provê uma interface que nos permite manipular dados de forma similar ao que faríamos utilizando uma tabela de Excel. Nos devolve uma estrutura de dados chamada [DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html), onde organizamos os dados em linhas e colunas.\n",
    "\n",
    "### MatPlotLib\n",
    "\n",
    "[MatPlotLib](https://matplotlib.org/) é uma biblioteca para plotagem de gráficos. Suas ferramentas permitem customização completa dos gráficos gerados.\n",
    "\n",
    "### Seaborn\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/) provê uma camada _high-level_ de abstração para a utilização da biblioteca Matplotlib, ou seja, é um facilitador.\n",
    "\n",
    "### Scikit-Learn\n",
    "\n",
    "[Scikit-Learn](http://scikit-learn.org/stable/) é uma das principais bibliotecas utilizadas para machine learning em Python, é open source e mantida por diversas instituições de ensino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ii_D683nWT7w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h46fOJ2LWT8E"
   },
   "source": [
    "### Importando os dados para uma estrutura de DataFrame\n",
    "\n",
    "A biblioteca Pandas possui métodos facilitadores para a importação de vários tipos de fontes de dados em um DataFrame. Neste laboratório vamos utilizar um arquivo no formato csv ([**C**omma-**S**eparated **V**alues](https://pt.wikipedia.org/wiki/Comma-separated_values)).\n",
    "\n",
    "Para ter uma visão de todos os facilitadores de importação como quais parâmetros de formatação podemos utilizar para importar estes dados, veja a [documentação da biblioteca](https://pandas.pydata.org/pandas-docs/stable/io.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Kamq6dBWT8H"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/pgiaeinstein/otmz-mlp/master/bcw.data.csv', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V36FlIZZWT8V"
   },
   "source": [
    "Veja que quando imprimo o dataframe completo ele me mostra uma quantidade de 60 itens sendo os 30 primeiros e os 30 últimos da coleção.\n",
    "\n",
    "Essa forma de visualizar nem sempre é necessária e pode poluir nossa documentação. Para uma visão mais controlada do Dataframe, podemos utilizar o método [`head()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html) que imprime, por padrão, as primeiras 5 linhas de informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4ZUvEm3WT8d"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EGGn4YdWT8o"
   },
   "source": [
    "O método aceita como argumento principal o número de linhas que desejamos imprimir, veja no exemplo abaixo quando solicitamos que as 32 linhas iniciais sejam impressas em nosso documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSOiA3V5WT8t"
   },
   "outputs": [],
   "source": [
    "df.head(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TVCcM_aWT85"
   },
   "source": [
    "### Modificando o dataframe\n",
    "\n",
    "Para uma análise inicial dos dados, outro método interessante é o [`info()`](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.info.html), este método imprime um resumo quantitativo e qualitativo além da estrutura completa de nosso Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDMw8FXAWT9A"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAWT0fYgWT9Q"
   },
   "source": [
    "Vamos entender o que é impresso acima:\n",
    "\n",
    "Temos 569 linhas de informação neste dataframe, iniciando no índice 0 até o índice 568. Nestas 569 entradas temos 33 atributos por linha, ou seja, temos 33 colunas.\n",
    "\n",
    "Verifique que o método sumariza para cada linha, o nome de sua coluna, o total de valores não nulos nesta coluna assim como também o tipo de dado que a coluna guarda.\n",
    "\n",
    "Repare na coluna `Unnamed: 32`; Está coluna possui 0 valores não nulos, ou seja, todos os valores desta coluna são nulos e devem ser desconsiderados pois não nos ajuda em nada neste laboratório.\n",
    "\n",
    "Além da coluna `Unnamed: 32`, este dataframe possui outra coluna `id` que não faz sentido para este laboratório.\n",
    "\n",
    "Por último, temos um resumo dos tipos de dados presentes no dataframe e também qual o tamanho em memória ocupado por este dataframe.\n",
    "\n",
    "O método [`drop()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html) permite eliminar colunas ou linhas do nosso dataframe, no caso, vamos remover as colunas `Unnamed: 32` e `id` como dito acima e utilizaremos o argumento `inplace = True` para modificar o objeto em que estamos utilizando o método.\n",
    "\n",
    "Por segurança, vamos criar uma cópia do objeto original e salvaremos na variável `df_inicial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4hRV8atqWT9U"
   },
   "outputs": [],
   "source": [
    "# vou criar uma cópia do dataframe inicial por segurança\n",
    "df_inicial = df\n",
    "\n",
    "# vamos remover as colunas\n",
    "df.drop(columns = ['id', 'Unnamed: 32'], inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKwtco1UWT9r"
   },
   "source": [
    "Reparem na coluna `diagnosis`, esta coluna é chamada de *TARGET*, ou seja, é a nossa coluna de saída para nosso modelo de classificação.\n",
    "\n",
    "Ela possui dois valores em formato `char` (**M** e **B**), para facilitar nossa vida, vamos modificar essa variável para um valor numérico.\n",
    "\n",
    "Criaremos um dicionário auxiliar para o método [`map()`](http://book.pythontips.com/en/latest/map_filter.html) onde vamos classificar a letra **B** (Benigno) como **0** e a letra **M** (Maligno) como **1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWVkl1oxWT9_"
   },
   "outputs": [],
   "source": [
    "label = {\n",
    "    'B' : 0,\n",
    "    'M' : 1\n",
    "}\n",
    "\n",
    "df['diagnosis'] = df['diagnosis'].map(label)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEif5Y-oWT-K"
   },
   "source": [
    "Depois desta transformação, agora nossa coluna de saída possui valores numéricos distintos (0 e 1).\n",
    "\n",
    "Vamos analisar de forma mais estatística nosso dataframe agora, para isso, utilizamos o método [`describe()`](https://pandas.pydata.org/pandas-docs/version/0.22/generated/pandas.DataFrame.describe.html).\n",
    "\n",
    "Este método retorna um resumo estatístico de nosso dataframe, por coluna, desconsiderando valores nulos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9UEeIFRWT-R"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPqZXWVOWT-e"
   },
   "source": [
    "### Escolhendo nossas Features\n",
    "\n",
    "Temos 3 tipos de medidas neste dataset: Mean, SE e Worst. É importante entender como estes valores se comunicam.\n",
    "\n",
    "Para alguns modelos clássicos utilizados em aprendizado de máquina, temos problemas quanto maior for o número de entradas e de colunas, característica esta que é o inverso quando comparada a um modelo de rede neural, por exemplo.\n",
    "\n",
    "Entendendo isso vamos escolher variáveis de *input* que beneficiem nossa tarefá de classificação, sem utilizar nenhum modelo auxiliar para selecionar estas variáveis.\n",
    "\n",
    "Separamos então nossas colunas em 3 tipos distintos: Mean, SE e Worst. Após essa separação, vamos analisar cada grupo buscando possíveis correlações entre estas variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPq8zQXcWT-n"
   },
   "outputs": [],
   "source": [
    "# vamos criar um dataframe excluindo a coluna 'diagnosis':\n",
    "df_parcial = df.iloc[:, 1:]\n",
    "df_parcial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8Jtf3yrWT-z"
   },
   "outputs": [],
   "source": [
    "# Neste novo dataframe, vamos listar as colunas que restaram:\n",
    "df_parcial.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLuNZHXGWT_F"
   },
   "source": [
    "Verifiquem que os 3 grupos de medidas estão em ordem, temos então levando como base o array acima:\n",
    "\n",
    "#### Variáveis do tipo MEAN:\n",
    "> Posição 0 até posição 9 do array.\n",
    "\n",
    "#### Variáveis do tipo SE:\n",
    "> Posição 10 até posição 19 do array.\n",
    "\n",
    "#### Variáveis do tipo WORST:\n",
    "> Posição 20 até posição 29 do array.\n",
    "\n",
    "Vamos separar nossas features por tipo, criando listas com as colunas de interesse para cada tipo de variável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6WBIeCdWT_I"
   },
   "outputs": [],
   "source": [
    "f_mean  = list(df_parcial.columns[:10])\n",
    "f_se    = list(df_parcial.columns[10:20])\n",
    "f_worst = list(df_parcial.columns[20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZY-iSJzWT_Y"
   },
   "outputs": [],
   "source": [
    "f_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMtpGrXxWT_t"
   },
   "outputs": [],
   "source": [
    "f_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6ORxHr2WT__"
   },
   "outputs": [],
   "source": [
    "f_worst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wK-JEJWUWUAQ"
   },
   "source": [
    "Outro aspecto importante que devemos sempre levar em consideração é como as colunas se correlacionam, uma correlação forte entre variáveis tende a divergir o resultado do modelo em alguns casos.\n",
    "\n",
    "Agora vamos obter a matriz de correlação destas variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4-IJEjDWUAU"
   },
   "outputs": [],
   "source": [
    "correlacao = df_parcial[f_mean].corr()\n",
    "correlacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IKUrdleWUAh"
   },
   "source": [
    "Lembrando que quanto maior a proximidade do valor entre 1 e -1, maior é a correlação entre as duas colunas para facilitar a visualização desta matriz, algo que é muito utilizado é um gráfico do tipo HeatMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SJKxLpx6WUAj"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "sns.heatmap(correlacao, xticklabels = f_mean, yticklabels= f_mean, cbar = True, square = True, annot = True, fmt = '.2f', annot_kws={ 'size' : 15}, cmap = 'winter', ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_dTq-lbWUAu"
   },
   "source": [
    "#### Interpretando o gráfico\n",
    "\n",
    "As observações aqui são simples, vamos remover de nossa base, todos os campos que tem forte correlação!\n",
    "\n",
    "Verifique que existem 2 \"quadrados\" onde é possível verificar forte correlação entre os campos: `radius_mean`, `perimeter_mean` e `area_mean` formam o primeiro \"quadrado\" e os campos `compactness_mean`, `concavity_mean` e `concavepoint_mean` formam o segundo.\n",
    "\n",
    "Destes dois conjuntos, escolhemos um de cada e seguimos com nossa analise.\n",
    "\n",
    "Do primeiro grupo, vemos que `area_mean` tem os menores valores de correlação com as demais colunas, vamos escolher esta *feature* neste conjunto.\n",
    "\n",
    "Do segundo conjunto, vemos que `compactness_mean` tem os menores valores, então seguiremos com ele.\n",
    "\n",
    "Nossa lista de colunas final será:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BUb3TkzWUA0"
   },
   "outputs": [],
   "source": [
    "features_mean = ['texture_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g56c2rILWUA9"
   },
   "source": [
    "#### Criando dataframes separados entre features e meta\n",
    "\n",
    "Como já temos nossas features iniciais, podemos agora criar dois vetores, um com nossas *features* escolhidas e outro chamado de *target*, ou seja, com a classificação para cada linha de nosso dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJ7EPlutWUBA"
   },
   "outputs": [],
   "source": [
    "features = df[features_mean]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26zDOsEGWUBZ"
   },
   "outputs": [],
   "source": [
    "target = df['diagnosis']\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dv77EhPAWUBi"
   },
   "source": [
    "#### Separando nossa amostra em treino e teste\n",
    "\n",
    "Vamos separar agora a nossa base entre uma base de treino e uma base de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RC1zkDm0WUBl"
   },
   "outputs": [],
   "source": [
    "seed = 4\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, test_size = 0.2, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivd5XQtjWUBv"
   },
   "source": [
    "Vamos criar duas funções que irão nos auxiliar com o treino, a predição e a exibição dos resultados de nossas predições."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xH_EqbMWWUBy"
   },
   "outputs": [],
   "source": [
    "def calcula_resultados(pred_output, real_output):\n",
    "    cm  = confusion_matrix(real_output, pred_output)\n",
    "    acc = accuracy_score(real_output, pred_output)\n",
    "    f1  = f1_score(real_output, pred_output)\n",
    "    ps  = precision_score(real_output, pred_output)\n",
    "    rs  = recall_score(real_output, pred_output)\n",
    "    \n",
    "    return {\n",
    "        'matrix'   : cm,\n",
    "        'accuracy' : acc,\n",
    "        'f1'       : f1,\n",
    "        'ps'       : ps,\n",
    "        'rs'       : rs\n",
    "    }\n",
    "\n",
    "def testa_modelo(modelo, X_train, X_test, Y_train, Y_test):\n",
    "    modelo.fit(X_train, Y_train)\n",
    "    pred_output = modelo.predict(X_test)\n",
    "    response = calcula_resultados(pred_output, Y_test)\n",
    "    \n",
    "    print('-----------------------------')\n",
    "    print('Accuracy : {}'.format(response['accuracy']))\n",
    "    print('F1 : {}'.format(response['f1']))\n",
    "    print('Precision : {}'.format(response['ps']))\n",
    "    print('Recall : {}'.format(response['rs']))\n",
    "    print('-----------------------------')\n",
    "    sns.heatmap(response['matrix'], annot = True, cmap = 'winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8j5MrycWUB9"
   },
   "source": [
    "### Modelos\n",
    "\n",
    "#### SVM (Support Vector Machine)\n",
    "\n",
    "Uma [**SVM**](https://pt.wikipedia.org/wiki/M%C3%A1quina_de_vetores_de_suporte) é um excelente método para se testar em primeiro lugar quando não se tem nenhum conhecimento prévio sobre um domínio. Três propriedades tornam a SVM atraente:\n",
    "\n",
    "1. Constroem um **separador de margem máxima**:\n",
    "\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/svm01.jpg)\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/svm02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UnocfpSsWUCA"
   },
   "source": [
    "2. Criam uma separação linear em hiperplano, mas tem a capacidade de entender dados em um espaço de dimensão superior, usando o **truque de kernel**.\n",
    "\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/svm03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2-URw8leWUCL"
   },
   "source": [
    "3. Uma SVM é **não paramétrica**, ou seja, existe a necessidade em guardar os exemplos de treinamento. Porém, na prática, acabam guardando apenas uma **pequena fração do número de exemplos**.\n",
    "\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/svm04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kH3sKX7NWUCO"
   },
   "outputs": [],
   "source": [
    "svc_model = SVC()\n",
    "testa_modelo(svc_model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbxBI8OhWUCX"
   },
   "source": [
    "#### Padronização dos dados\n",
    "\n",
    "Já discutimos que alguns algoritmos sofrem com dados em escalas que divergem muito, vamos ver este conceito na prática.\n",
    "\n",
    "Reparem na distribuição de nossas features atualmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbXOHfoaWUCc"
   },
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVSwfEhgWUCp"
   },
   "source": [
    "A padronização realiza a seguinte operação:\n",
    "    \n",
    "$$ \n",
    "X_i = \\frac{X_i \\times \\overline{X}}{std_X}\n",
    "$$\n",
    "\n",
    "Basicamente o que estamos realizando é ignorar a distribuição original da nossa base. Transformaremos os dados para obter uma média muito próxima de 0 e desvio padrão próximo de 1, sendo assim não teremos valores com grande variância na nossa base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlBG8PESWUCt"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6FMNGypWUC0"
   },
   "outputs": [],
   "source": [
    "X_train_scaler_df = pd.DataFrame(X_train_scaler, columns = X_train.columns)\n",
    "X_train_scaler_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rWtrpFLiWUDA"
   },
   "outputs": [],
   "source": [
    "testa_modelo(svc_model, X_train_scaler, X_test_scaler, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMMbQ0ORWUDX"
   },
   "source": [
    "#### Árvore de Decisão\n",
    "\n",
    "Uma **Árvore de Decisão** representa uma função que recebe em seus parâmetros de entrada um vetor de valores e retorna uma resposta / classificação.\n",
    "\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/DT01.png)\n",
    "\n",
    "Uma árvore alcança sua resposta executando uma sequência de testes onde cada nó interno de sua estrutura corresponde a um teste do valor e de um dos atributos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bb69TVC0WUDb"
   },
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "\n",
    "DT_model = DecisionTreeClassifier(random_state = random_state)\n",
    "testa_modelo(DT_model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fscLIzQ2WUDj"
   },
   "outputs": [],
   "source": [
    "testa_modelo(DT_model, X_train_scaler, X_test_scaler, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adoWRLl4WUDs"
   },
   "source": [
    "#### kNN (K Nearest-Neighbor)\n",
    "\n",
    "O **kNN** ([k-Vizinhos Mais Próximos](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)) é um algoritmo não linear que literalmente mede a distância de um determinado ponto sem classificação em relação a k pontos conhecidos.\n",
    "\n",
    "A classificação deste ponto então se dá pelo maior número de similares dentre os k vizinhos mais próximos analisados:\n",
    "\n",
    "![SVM01](https://github.com/pgiaeinstein/otmz-mlp/raw/master/img/knn01.png)\n",
    "\n",
    "A distância pode ser calculada de vários modos, o mais comum é utilizar a distância euclidiana, que respeita a seguinte equação:\n",
    "\n",
    "$$\n",
    "D_{A,B} = \\sqrt{(A_1 - B_2)^2+(A_2 - B_2)^2+\\ldots+(B_n - B_n)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXPp_McpWUDv"
   },
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "testa_modelo(knn_model, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mHNfALRoWUD6"
   },
   "outputs": [],
   "source": [
    "testa_modelo(knn_model, X_train_scaler, X_test_scaler, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fygouq4KWUEC"
   },
   "source": [
    "Novamente vemos um resultado melhor quando utilizamos os dados padronizados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfmwplz_WUEE"
   },
   "source": [
    "## Exercício 2\n",
    "\n",
    "Crie uma rede neural para classificar o problema proposto no **Lab01**, compare os valores obtidos anteriormente com o melhor valor encontrado em sua rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCUrh6UiWUEG"
   },
   "outputs": [],
   "source": [
    "# Resolução\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZGwqmIhBWUEN"
   },
   "source": [
    "### Exercício 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1DrUXIOWUEQ"
   },
   "outputs": [],
   "source": [
    "data_ns = pd.read_csv('https://raw.githubusercontent.com/pgiaeinstein/otmz-mlp/master/base_ns.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCl1CminWUEb"
   },
   "outputs": [],
   "source": [
    "data_ns.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Atividade_1.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb",
     "timestamp": 1541690672629
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
