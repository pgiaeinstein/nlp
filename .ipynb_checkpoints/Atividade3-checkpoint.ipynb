{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# Atividade 3\n",
    "\n",
    "## Análise e classificação de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg62Pmz3o83v"
   },
   "source": [
    "Nesta atividade iremos utilizar um _dataset_ chamado __IMDB__ composto de opiniões sobre filmes em formato textual. O objetivo é classificar estes _inputs_ de forma binária (0 e 1) onde identificaremos se a opinião é positiva ou negativa.\n",
    "\n",
    "__IMDB__ (Internet Movie DataBase) conta com um total de 50.000 opiniões sobre filmes onde separamos 25.000 delas para treino e os 50% restantes para teste. As frações são balanceadas, ou seja, contem um número igual de opiniões positivas e negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAsKG535pHep"
   },
   "source": [
    "## Importando a base para o projeto\n",
    "\n",
    "Podemos obter os dados utilizando o facilitador `keras.datasets`, devemos lembrar que as palavras já foram trabalhadas e estão num formato numérico. \n",
    "\n",
    "Vamos apenas buscar as 10.000 palavras mais frequentes para facilitar no processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "qtd_palavras = 10000\n",
    "\n",
    "dataset = keras.datasets.imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = dataset.load_data(num_words = qtd_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## Visualizando os dados\n",
    "\n",
    "Vamos entender o formato da informação que iremos utilizar para o trabalho. Esse é um passo principal e deve sempre ser realizado em qualquer processo analítico. Entender o formato do dado e qual é a importância de cada parcela da informação é primordial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8qCnve_-lkO"
   },
   "outputs": [],
   "source": [
    "qtd_base_treino = len(X_train)\n",
    "qtd_meta_treino = len(y_train)\n",
    "qtd_base_teste  = len(X_test)\n",
    "qtd_meta_teste  = len(y_test)\n",
    "\n",
    "txt = '''\\\n",
    "Quantidade de entradas e saídas para treino: {0} - {1}\n",
    "Quantidade de entradas e saídas para teste:  {2} - {3}\n",
    "'''.format(qtd_base_treino, qtd_meta_treino, qtd_base_teste, qtd_meta_teste)\n",
    "\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "Como foi dito acima, as palavras já foram transformadas em uma representação numérica, vamos imprimir o primeiro índice do array para ter uma ideia do formato do dado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4wJg2FiYpuoX"
   },
   "source": [
    "### Numerais para palavras\n",
    "\n",
    "Podemos criar uma função que recebe de entrara um vetor e retorna o mesmo vetor trocando os numerais por seu representativo textual.\n",
    "\n",
    "O método `get_word_index()` retorna o dicionário (chave : valor) que faz a tradução das palavras. Os textos tem algumas _tags_ que são utilizadas como auxiliares na etapa inicial do processamento do texto, são elas: `<PAD>`, `<START>`, `<UNK>` e `<UNUSED>`. As posições iniciais do dicionário de palavras são reservadas para as _tags_, devemos levar isso em consideração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tr5s_1alpzop"
   },
   "outputs": [],
   "source": [
    "dic_palavras = dataset.get_word_index()\n",
    "\n",
    "dic_palavras = {chave: (valor + 3) for chave, valor in dic_palavras.items()} \n",
    "\n",
    "dic_palavras[\"<PAD>\"]    = 0\n",
    "dic_palavras[\"<START>\"]  = 1\n",
    "dic_palavras[\"<UNK>\"]    = 2\n",
    "dic_palavras[\"<UNUSED>\"] = 3\n",
    "\n",
    "dic_palavras_formatado = dict([(valor, chave) for (chave, valor) in dic_palavras.items()])\n",
    "\n",
    "def decode_palavras(vetor_palavras):\n",
    "    return ' '.join([dic_palavras_formatado.get(palavra, '?') for palavra in vetor_palavras])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3CNRvEZVppl"
   },
   "source": [
    "Vamos utilizar nossa função auxiliar `decode_palavras()` para traduzir o primeiro índice de nosso conjunto de treino `X_train`, veja o resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s_OqxmH6-lkn"
   },
   "outputs": [],
   "source": [
    "decode_palavras(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PnBepIciZ5sm"
   },
   "source": [
    "Verifique o texto de saída:\n",
    "\n",
    "```\n",
    "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\"\n",
    "```\n",
    "\n",
    "Note que as _tags_ auxiliam a formatação do conteúdo, palavras que não foram corretamente processadas na etapa inicial do processo ou vieram com algum tipo de \"sujeira\" foram trocadas pela _tag_ `<UNK>` de _unknow_ (desconhecido).\n",
    "\n",
    "Tradicionalmente, o processamento de texto inicial envolve um processo de remoção das _stopwords_, conversão das palavras para caixa baixa, remoção de pontuação e no caso do português, remoção da acentuação gráfica de palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFP_XKVRp4_S"
   },
   "source": [
    "## Preparando os vetores para a batalha\n",
    "\n",
    "Sabemos que uma rede neural espera um tensor de entrada, e vamos verificar uma coisa antes de continuar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR4y-UIxekEE"
   },
   "outputs": [],
   "source": [
    "print('{} - {} - {}'.format(len(X_train[0]), len(X_train[1]), len(X_train[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4Ga1N2mrAbM"
   },
   "source": [
    "Verifique que imprimimos o tamanho dos vetores em `X_train` nas posições 0, 1 e 2. Os valores que obtivemos foram 218, 189 e 141 respectivamente.\n",
    "\n",
    "Temos um problema pois uma rede neural espera um tensor de tamanho único na entrada. Podemos resolver este problema de algumas formas, por exemplo:\n",
    "\n",
    "* __OneHot-Encode__: Os vetores serão convertidos em vetores binários (0 e 1) de dimensão igual ao número total de palavras que temos em nosso dicionário de palavras. Há benefícios em relação a velocidade na busca dos valores transformados, porém, existe um alto uso de memória já que teremos uma matriz de dimensão $\\text{número de palavras} \\times \\text{número de opiniões}$.\n",
    "* __PadSequence__: Os vetores são preenchidos com de forma que tenham o tamanho do maior vetor da nossa coleção de documentos, ou seja, teremos um tensor de dimensão $\\text{tamanho do maior vetor} \\times \\text{número de opiniões}$. Podemos criar uma camada de _embedding_ como camada inicial de nossa rede para processar essa informação de entrada, algo similar ao que vimos no exemplo do _Word2Vec_.\n",
    "\n",
    "A biblioteca _tensorflow_ tem um facilitador para aplicar [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) aos vetores, vamos utilizar essa abordagem para padronizar o tamanho dos vetores de _input_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GgrzblRjsYhj"
   },
   "outputs": [],
   "source": [
    "len(X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A_NYkOysaOC"
   },
   "outputs": [],
   "source": [
    "len(X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "input_dim = 894\n",
    "\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                     value = dic_palavras[\"<PAD>\"],\n",
    "                                                     padding = 'post',\n",
    "                                                     maxlen = input_dim)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                    value = dic_palavras[\"<PAD>\"],\n",
    "                                                    padding = 'post',\n",
    "                                                    maxlen = input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VO5MBpyQdipD"
   },
   "source": [
    "Vamos verificar as dimensões após a operação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USSSBnkE-lky"
   },
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TG8X9cqi-lk9"
   },
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## Construindo o modelo\n",
    "\n",
    "Lembre que nossa primeira camada será uma camada de `embedding` que deve ser capaz de traduzir todas as palavras de nosso dicionário de palavras, por isso, esta camada deve ter uma dimensão de entrada igual ao número total de palavras no nosso dicionário.\n",
    "\n",
    "Utilizaremos uma camada convolucional de uma dimensão para criar um vetor representativo da nossa informação, lembre que teremos uma representação vetorial para cada palavra agora.\n",
    "\n",
    "A camada de saída dessa rede deve resultar em um valor binário (0 e 1), por este motivo, vamos utilizar uma função `sigmoid` na saída da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpKOoWgu-llD"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(qtd_palavras, 16, input_length = input_dim))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation = tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation = tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### Função loss\n",
    "\n",
    "Como estamos utilizando uma função `sigmoid` na camada de saída, vamos utilizar `binary_crossentropy` como função de loss. Essa função calcula qual é a distância entre o que o modelo deveria ter dado como resposta e a sua atual resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## Treino\n",
    "\n",
    "Vamos treinar o modelo por 60 épocas em batches de 512 amostras. Isso quer dizer que em 60 iterações o modelo vai receber todos os tensores de `X_train` e vai corrigir a saída comparando o resultado com o que existe em `y_train`, 512 exemplos por vez. Enquanto estas 60 iterações ocorrem, vamos testar o modelo com 20% dos tensores de `X_train`, essa é a nossa parcela de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs = 60,\n",
    "                    batch_size = 512,\n",
    "                    validation_split = 0.2,\n",
    "                    verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## Hora da verdade\n",
    "\n",
    "Através do método `evaluate(input, output)` é possível validar como o modelo está respondendo a um determinado conjunto de dados. No nosso caso, vamos verificar como o modelo se comporta com a parcela de informação que temos em `X_test`.\n",
    "\n",
    "O método retorna um vetor, na primeira posição temos o valor calculado para o loss do modelo e na segunda posição a acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOMKywn4zReN"
   },
   "outputs": [],
   "source": [
    "resultados = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(resultados)\n",
    "print('-------- * --------')\n",
    "print('Loss teste: {:.4f}'.format(resultados[0]))\n",
    "print(' Acc teste: {:.2f}%'.format(resultados[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## Plot dos resultados de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGoYf2Js-lle"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(1, figsize = (15, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss, 'r', label = 'Loss treino')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Loss validação')\n",
    "plt.title('Loss: Treino e validação')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc, 'r', label = 'Acc treino')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'acc validação')\n",
    "plt.title('Acc: Treino e validação')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZXYwAR69Zds"
   },
   "source": [
    "# RNN\n",
    "\n",
    "Vamos resolver o mesmo problema acima, desta vez com uma RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vz4Dxag89dAp"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout\n",
    "from tensorflow.keras.layers import LSTM, CuDNNLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBDXzNGjfsum"
   },
   "source": [
    "Vamos reduzir a quantidade de palavras de entrara por uma questão de recursos para processamento na plataforma do Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4-SwVbVlnFg"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = dataset.load_data(num_words = qtd_palavras)\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen = 80)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZnV2Ua1Xf10S"
   },
   "source": [
    "O modelo é construído da mesma forma, uma novidade é a camada `LSTM`.\n",
    "\n",
    "Essa classe da biblioteca `Keras` implementa uma célula de memória e podemos \"empilhar\" essas células. Sempre lembrando que todas, exceto a ultima da cadeia, deve ter o parâmetro `return_sequences = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iRE5siA9--E"
   },
   "outputs": [],
   "source": [
    "model_RNN = Sequential()\n",
    "model_RNN.add(Embedding(qtd_palavras, 50, input_length = 80))\n",
    "# model_RNN.add(LSTM(32))\n",
    "model_RNN.add(CuDNNLSTM(32)) # Caso GPU\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#model_RNN.add(LSTM(32, return_sequences=True))\n",
    "#model_RNN.add(LSTM(32, return_sequences=True))\n",
    "#model_RNN.add(LSTM(32))\n",
    "model_RNN.add(Dense(1, activation = tf.nn.sigmoid))\n",
    "\n",
    "model_RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hZRSuO6--WN"
   },
   "outputs": [],
   "source": [
    "model_RNN.compile(optimizer = tf.train.AdamOptimizer(),\n",
    "                  loss = 'binary_crossentropy',\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu-gefrN_GBD"
   },
   "outputs": [],
   "source": [
    "history_RNN = model_RNN.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs = 5,\n",
    "                            batch_size = 128,\n",
    "                            validation_split = 0.2,\n",
    "                            verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_knPmD5x_USB"
   },
   "outputs": [],
   "source": [
    "resultados_RNN = model_RNN.evaluate(X_test, y_test)\n",
    "\n",
    "print(resultados_RNN)\n",
    "print('-------- * --------')\n",
    "print('Loss teste: {:.4f}'.format(resultados_RNN[0]))\n",
    "print(' Acc teste: {:.2f}%'.format(resultados_RNN[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFXD1ySj_eFl"
   },
   "outputs": [],
   "source": [
    "history_dict_RNN = history_RNN.history\n",
    "\n",
    "acc = history_RNN.history['acc']\n",
    "val_acc = history_RNN.history['val_acc']\n",
    "loss = history_RNN.history['loss']\n",
    "val_loss = history_RNN.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(1, figsize = (15, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(epochs, loss, 'r', label = 'Loss treino')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Loss validação')\n",
    "plt.title('Loss: Treino e validação')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.subplot(122)\n",
    "plt.plot(epochs, acc, 'r', label = 'Acc treino')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'acc validação')\n",
    "plt.title('Acc: Treino e validação')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Atividade_1.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/basic_text_classification.ipynb",
     "timestamp": 1541690672629
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
