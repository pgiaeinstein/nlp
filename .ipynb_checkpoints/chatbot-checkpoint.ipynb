{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\secol\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.10) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from unidecode import unidecode \n",
    "\n",
    "import colorama\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de conhecimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conhecimento = {\n",
    "    'base' : [\n",
    "        {\n",
    "            'label' : 'INIT',\n",
    "            'X'     : ['Olá', 'Oi', 'bom dia', 'boa tarde', 'boa noite', 'como vai?', 'tem alguem ai?'],\n",
    "            'Y'     : ['olá', 'como vai?', 'como posso ajudar?']\n",
    "        },\n",
    "        {\n",
    "            'label' : 'END',\n",
    "            'X'     : ['tchau', 'até logo', 'obrigado pela ajuda'],\n",
    "            'Y'     : ['Sem problemas!', 'Meu prazer!']\n",
    "        },\n",
    "        {\n",
    "            'label' : 'BOT_INFO',\n",
    "            'X'     : ['qual e o seu nome?', 'como posso te chamar?', 'com quem falo?'],\n",
    "            'Y'     : ['Você pode me chamar de rose!', 'Meu nome e rose!', 'Me chamo rose!']\n",
    "        },\n",
    "        {\n",
    "            'label' : 'BOT_SOBRE',\n",
    "            'X'     : ['quem e voce?', 'fale sobre você', 'pode se apresentar?'],\n",
    "            'Y'     : ['Siri é passado! sou a Rose, seu robo assistente!', 'Meu nome é Rose, sou um chatbot!']\n",
    "        },\n",
    "        {\n",
    "            'label' : 'BOT_AGRADECIMENTO',\n",
    "            'X'     : ['obrigado', 'muito obrigado'],\n",
    "            'Y'     : ['Foi um prazer ajudar!', 'Obrigado!']\n",
    "        },\n",
    "        {\n",
    "            'label' : 'BOT_AGENDAMENTO',\n",
    "            'X'     : ['quero agendar um exame', 'gostaria de saber sobre a disponibilidade de agenda para um exame', 'posso agendar um exame?', 'preciso realizar um agendamento de exame'],\n",
    "            'Y'     : ['Ok, qual exame precisa agendar?', 'Certo! Preciso de mais informações. Qual exame gostaria de agendar?']\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list()\n",
    "y_train = list()\n",
    "labels = list()\n",
    "target = dict()\n",
    "\n",
    "for item in base_conhecimento['base']:\n",
    "    for x in item['X']:\n",
    "        x_train.append(unidecode(x.lower()))\n",
    "        y_train.append(item[ 'label'])\n",
    "    \n",
    "    if item['label'] not in labels:\n",
    "        labels.append(item['label'])\n",
    "        \n",
    "    target[item['label']] = item['Y']\n",
    "    \n",
    "n_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ola', 'oi', 'bom dia', 'boa tarde', 'boa noite']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INIT', 'INIT', 'INIT', 'INIT', 'INIT']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INIT', 'END', 'BOT_INFO', 'BOT_SOBRE', 'BOT_AGRADECIMENTO']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INIT': ['olá', 'como vai?', 'como posso ajudar?'],\n",
       " 'END': ['Sem problemas!', 'Meu prazer!'],\n",
       " 'BOT_INFO': ['Você pode me chamar de rose!',\n",
       "  'Meu nome e rose!',\n",
       "  'Me chamo rose!'],\n",
       " 'BOT_SOBRE': ['Siri é passado! sou a Rose, seu robo assistente!',\n",
       "  'Meu nome é Rose, sou um chatbot!'],\n",
       " 'BOT_AGRADECIMENTO': ['Foi um prazer ajudar!', 'Obrigado!'],\n",
       " 'BOT_AGENDAMENTO': ['Ok, qual exame precisa agendar?',\n",
       "  'Certo! Preciso de mais informações. Qual exame gostaria de agendar?']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando rótulos para cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOT_AGENDAMENTO', 'BOT_AGRADECIMENTO', 'BOT_INFO', 'BOT_SOBRE',\n",
       "       'END', 'INIT'], dtype='<U17')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando uma nova estrutura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25\n",
    "embedding_dim = 32\n",
    "max_len = 50\n",
    "unk_token = '<UNK>'\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=unk_token)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
    "\n",
    "words_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14], [15], [16, 17], [6, 18], [6, 19]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, 14],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0, 15],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        16, 17],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         6, 18],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         6, 19]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 32)            800       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_4 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 6)                 102       \n",
      "=================================================================\n",
      "Total params: 3,542\n",
      "Trainable params: 3,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "22/22 [==============================] - 0s 5ms/sample - loss: 1.7909 - acc: 0.2273\n",
      "Epoch 2/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7889 - acc: 0.3182\n",
      "Epoch 3/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7874 - acc: 0.3182\n",
      "Epoch 4/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7859 - acc: 0.3182\n",
      "Epoch 5/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7846 - acc: 0.3182\n",
      "Epoch 6/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7831 - acc: 0.3182\n",
      "Epoch 7/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7816 - acc: 0.3182\n",
      "Epoch 8/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7802 - acc: 0.3182\n",
      "Epoch 9/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.7785 - acc: 0.3182\n",
      "Epoch 10/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7767 - acc: 0.3182\n",
      "Epoch 11/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7749 - acc: 0.3182\n",
      "Epoch 12/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7730 - acc: 0.3182\n",
      "Epoch 13/250\n",
      "22/22 [==============================] - 0s 111us/sample - loss: 1.7711 - acc: 0.3182\n",
      "Epoch 14/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7692 - acc: 0.3182\n",
      "Epoch 15/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.7672 - acc: 0.3182\n",
      "Epoch 16/250\n",
      "22/22 [==============================] - 0s 65us/sample - loss: 1.7651 - acc: 0.3182\n",
      "Epoch 17/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7631 - acc: 0.3182\n",
      "Epoch 18/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7610 - acc: 0.3182\n",
      "Epoch 19/250\n",
      "22/22 [==============================] - 0s 136us/sample - loss: 1.7588 - acc: 0.3182\n",
      "Epoch 20/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7566 - acc: 0.3182\n",
      "Epoch 21/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7543 - acc: 0.3182\n",
      "Epoch 22/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7520 - acc: 0.3182\n",
      "Epoch 23/250\n",
      "22/22 [==============================] - 0s 92us/sample - loss: 1.7497 - acc: 0.3182\n",
      "Epoch 24/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7474 - acc: 0.3182\n",
      "Epoch 25/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7451 - acc: 0.3182\n",
      "Epoch 26/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7427 - acc: 0.3182\n",
      "Epoch 27/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7404 - acc: 0.3182\n",
      "Epoch 28/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7381 - acc: 0.3182\n",
      "Epoch 29/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7358 - acc: 0.3182\n",
      "Epoch 30/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7335 - acc: 0.3182\n",
      "Epoch 31/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7313 - acc: 0.3182\n",
      "Epoch 32/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7291 - acc: 0.3182\n",
      "Epoch 33/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7270 - acc: 0.3182\n",
      "Epoch 34/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.7249 - acc: 0.3182\n",
      "Epoch 35/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.7229 - acc: 0.3182\n",
      "Epoch 36/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.7210 - acc: 0.3182\n",
      "Epoch 37/250\n",
      "22/22 [==============================] - 0s 69us/sample - loss: 1.7192 - acc: 0.3182\n",
      "Epoch 38/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7175 - acc: 0.3182\n",
      "Epoch 39/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7159 - acc: 0.3182\n",
      "Epoch 40/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7144 - acc: 0.3182\n",
      "Epoch 41/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7130 - acc: 0.3182\n",
      "Epoch 42/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7117 - acc: 0.3182\n",
      "Epoch 43/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7104 - acc: 0.3182\n",
      "Epoch 44/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7093 - acc: 0.3182\n",
      "Epoch 45/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7082 - acc: 0.3182\n",
      "Epoch 46/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7072 - acc: 0.3182\n",
      "Epoch 47/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7062 - acc: 0.3182\n",
      "Epoch 48/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7052 - acc: 0.3182\n",
      "Epoch 49/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7041 - acc: 0.3182\n",
      "Epoch 50/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7030 - acc: 0.3182\n",
      "Epoch 51/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7019 - acc: 0.3182\n",
      "Epoch 52/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.7008 - acc: 0.3182\n",
      "Epoch 53/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6997 - acc: 0.3182\n",
      "Epoch 54/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6986 - acc: 0.3182\n",
      "Epoch 55/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6975 - acc: 0.3182\n",
      "Epoch 56/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6964 - acc: 0.3182\n",
      "Epoch 57/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6954 - acc: 0.3182\n",
      "Epoch 58/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6944 - acc: 0.3182\n",
      "Epoch 59/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6933 - acc: 0.3182\n",
      "Epoch 60/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6923 - acc: 0.3182\n",
      "Epoch 61/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6914 - acc: 0.3182\n",
      "Epoch 62/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6905 - acc: 0.3182\n",
      "Epoch 63/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6895 - acc: 0.3182\n",
      "Epoch 64/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6885 - acc: 0.3182\n",
      "Epoch 65/250\n",
      "22/22 [==============================] - 0s 92us/sample - loss: 1.6876 - acc: 0.3182\n",
      "Epoch 66/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6866 - acc: 0.3182\n",
      "Epoch 67/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6855 - acc: 0.3182\n",
      "Epoch 68/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6844 - acc: 0.3182\n",
      "Epoch 69/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6833 - acc: 0.3182\n",
      "Epoch 70/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6822 - acc: 0.3182\n",
      "Epoch 71/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6811 - acc: 0.3182\n",
      "Epoch 72/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6799 - acc: 0.3182\n",
      "Epoch 73/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6787 - acc: 0.3182\n",
      "Epoch 74/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6774 - acc: 0.3182\n",
      "Epoch 75/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6761 - acc: 0.3182\n",
      "Epoch 76/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6748 - acc: 0.3182\n",
      "Epoch 77/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6734 - acc: 0.3182\n",
      "Epoch 78/250\n",
      "22/22 [==============================] - 0s 72us/sample - loss: 1.6720 - acc: 0.3182\n",
      "Epoch 79/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6705 - acc: 0.3182\n",
      "Epoch 80/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6690 - acc: 0.3182\n",
      "Epoch 81/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6674 - acc: 0.3182\n",
      "Epoch 82/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6657 - acc: 0.3182\n",
      "Epoch 83/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6640 - acc: 0.3182\n",
      "Epoch 84/250\n",
      "22/22 [==============================] - 0s 72us/sample - loss: 1.6621 - acc: 0.3182\n",
      "Epoch 85/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6603 - acc: 0.3182\n",
      "Epoch 86/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6583 - acc: 0.3182\n",
      "Epoch 87/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6562 - acc: 0.3182\n",
      "Epoch 88/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6540 - acc: 0.3182\n",
      "Epoch 89/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6516 - acc: 0.3182\n",
      "Epoch 90/250\n",
      "22/22 [==============================] - 0s 69us/sample - loss: 1.6491 - acc: 0.3182\n",
      "Epoch 91/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6463 - acc: 0.3182\n",
      "Epoch 92/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6435 - acc: 0.3182\n",
      "Epoch 93/250\n",
      "22/22 [==============================] - 0s 136us/sample - loss: 1.6407 - acc: 0.3182\n",
      "Epoch 94/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6379 - acc: 0.3182\n",
      "Epoch 95/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6350 - acc: 0.3182\n",
      "Epoch 96/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6318 - acc: 0.3182\n",
      "Epoch 97/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6286 - acc: 0.3182\n",
      "Epoch 98/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6252 - acc: 0.3182\n",
      "Epoch 99/250\n",
      "22/22 [==============================] - 0s 100us/sample - loss: 1.6216 - acc: 0.3182\n",
      "Epoch 100/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6179 - acc: 0.3182\n",
      "Epoch 101/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6140 - acc: 0.3182\n",
      "Epoch 102/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.6100 - acc: 0.3182\n",
      "Epoch 103/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.6058 - acc: 0.3636\n",
      "Epoch 104/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.6016 - acc: 0.3636\n",
      "Epoch 105/250\n",
      "22/22 [==============================] - 0s 47us/sample - loss: 1.5972 - acc: 0.3636\n",
      "Epoch 106/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.5927 - acc: 0.3636\n",
      "Epoch 107/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5879 - acc: 0.3636\n",
      "Epoch 108/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5830 - acc: 0.3636\n",
      "Epoch 109/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5777 - acc: 0.3636\n",
      "Epoch 110/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.5721 - acc: 0.3636\n",
      "Epoch 111/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5662 - acc: 0.3636\n",
      "Epoch 112/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5603 - acc: 0.3636\n",
      "Epoch 113/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5541 - acc: 0.3636\n",
      "Epoch 114/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5475 - acc: 0.3636\n",
      "Epoch 115/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5408 - acc: 0.3636\n",
      "Epoch 116/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.5339 - acc: 0.3636\n",
      "Epoch 117/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.5267 - acc: 0.3636\n",
      "Epoch 118/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5194 - acc: 0.4091\n",
      "Epoch 119/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5119 - acc: 0.4091\n",
      "Epoch 120/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.5043 - acc: 0.4091\n",
      "Epoch 121/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.4963 - acc: 0.4091\n",
      "Epoch 122/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.4880 - acc: 0.4091\n",
      "Epoch 123/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.4798 - acc: 0.4091\n",
      "Epoch 124/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.4713 - acc: 0.4091\n",
      "Epoch 125/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.4623 - acc: 0.4091\n",
      "Epoch 126/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.4528 - acc: 0.4091\n",
      "Epoch 127/250\n",
      "22/22 [==============================] - 0s 35us/sample - loss: 1.4432 - acc: 0.4545\n",
      "Epoch 128/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.4333 - acc: 0.4545\n",
      "Epoch 129/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.4232 - acc: 0.5000\n",
      "Epoch 130/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.4128 - acc: 0.5000\n",
      "Epoch 131/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.4024 - acc: 0.5000\n",
      "Epoch 132/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.3919 - acc: 0.5000\n",
      "Epoch 133/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.3816 - acc: 0.5000\n",
      "Epoch 134/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.3710 - acc: 0.5000\n",
      "Epoch 135/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.3607 - acc: 0.5000\n",
      "Epoch 136/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.3500 - acc: 0.5000\n",
      "Epoch 137/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.3394 - acc: 0.5000\n",
      "Epoch 138/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.3288 - acc: 0.5000\n",
      "Epoch 139/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.3178 - acc: 0.5000\n",
      "Epoch 140/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.3067 - acc: 0.5000\n",
      "Epoch 141/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2953 - acc: 0.5000\n",
      "Epoch 142/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2843 - acc: 0.5000\n",
      "Epoch 143/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.2731 - acc: 0.5000\n",
      "Epoch 144/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2614 - acc: 0.5000\n",
      "Epoch 145/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.2500 - acc: 0.5000\n",
      "Epoch 146/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.2388 - acc: 0.5000\n",
      "Epoch 147/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2271 - acc: 0.5000\n",
      "Epoch 148/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2155 - acc: 0.5000\n",
      "Epoch 149/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.2040 - acc: 0.5000\n",
      "Epoch 150/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.1922 - acc: 0.5000\n",
      "Epoch 151/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.1802 - acc: 0.5000\n",
      "Epoch 152/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.1684 - acc: 0.5000\n",
      "Epoch 153/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.1569 - acc: 0.5000\n",
      "Epoch 154/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.1462 - acc: 0.5000\n",
      "Epoch 155/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.1358 - acc: 0.5000\n",
      "Epoch 156/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.1259 - acc: 0.5000\n",
      "Epoch 157/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.1160 - acc: 0.5000\n",
      "Epoch 158/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 1.1066 - acc: 0.5000\n",
      "Epoch 159/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0972 - acc: 0.5000\n",
      "Epoch 160/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.0882 - acc: 0.5000\n",
      "Epoch 161/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0797 - acc: 0.5000\n",
      "Epoch 162/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0709 - acc: 0.5000\n",
      "Epoch 163/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 1.0628 - acc: 0.5000\n",
      "Epoch 164/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0547 - acc: 0.5000\n",
      "Epoch 165/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0470 - acc: 0.5000\n",
      "Epoch 166/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0396 - acc: 0.5000\n",
      "Epoch 167/250\n",
      "22/22 [==============================] - 0s 62us/sample - loss: 1.0322 - acc: 0.5000\n",
      "Epoch 168/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0251 - acc: 0.5000\n",
      "Epoch 169/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0179 - acc: 0.5000\n",
      "Epoch 170/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0109 - acc: 0.5000\n",
      "Epoch 171/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 1.0041 - acc: 0.5000\n",
      "Epoch 172/250\n",
      "22/22 [==============================] - 0s 111us/sample - loss: 0.9971 - acc: 0.5000\n",
      "Epoch 173/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.9907 - acc: 0.5000\n",
      "Epoch 174/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9840 - acc: 0.5000\n",
      "Epoch 175/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9777 - acc: 0.5000\n",
      "Epoch 176/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.9713 - acc: 0.5000\n",
      "Epoch 177/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.9652 - acc: 0.5000\n",
      "Epoch 178/250\n",
      "22/22 [==============================] - 0s 95us/sample - loss: 0.9592 - acc: 0.5000\n",
      "Epoch 179/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.9536 - acc: 0.5000\n",
      "Epoch 180/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.9476 - acc: 0.5455\n",
      "Epoch 181/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9423 - acc: 0.5000\n",
      "Epoch 182/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.9372 - acc: 0.5455\n",
      "Epoch 183/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9318 - acc: 0.5455\n",
      "Epoch 184/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9271 - acc: 0.5455\n",
      "Epoch 185/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9219 - acc: 0.5455\n",
      "Epoch 186/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.9170 - acc: 0.5455\n",
      "Epoch 187/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9120 - acc: 0.5455\n",
      "Epoch 188/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9076 - acc: 0.5455\n",
      "Epoch 189/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.9028 - acc: 0.5909\n",
      "Epoch 190/250\n",
      "22/22 [==============================] - 0s 67us/sample - loss: 0.8982 - acc: 0.5455\n",
      "Epoch 191/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8942 - acc: 0.5909\n",
      "Epoch 192/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8896 - acc: 0.6364\n",
      "Epoch 193/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8857 - acc: 0.6818\n",
      "Epoch 194/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.8807 - acc: 0.6818\n",
      "Epoch 195/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8769 - acc: 0.6364\n",
      "Epoch 196/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8718 - acc: 0.6818\n",
      "Epoch 197/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8678 - acc: 0.6818\n",
      "Epoch 198/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8634 - acc: 0.6818\n",
      "Epoch 199/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.8598 - acc: 0.6364\n",
      "Epoch 200/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.8556 - acc: 0.7273\n",
      "Epoch 201/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.8509 - acc: 0.7273\n",
      "Epoch 202/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8461 - acc: 0.7273\n",
      "Epoch 203/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.8434 - acc: 0.7273\n",
      "Epoch 204/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8383 - acc: 0.7727\n",
      "Epoch 205/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.8340 - acc: 0.7727\n",
      "Epoch 206/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.8295 - acc: 0.7727\n",
      "Epoch 207/250\n",
      "22/22 [==============================] - 0s 73us/sample - loss: 0.8255 - acc: 0.7727\n",
      "Epoch 208/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.8214 - acc: 0.7727\n",
      "Epoch 209/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.8171 - acc: 0.7727\n",
      "Epoch 210/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.8124 - acc: 0.7727\n",
      "Epoch 211/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.8085 - acc: 0.7727\n",
      "Epoch 212/250\n",
      "22/22 [==============================] - 0s 49us/sample - loss: 0.8041 - acc: 0.7727\n",
      "Epoch 213/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7989 - acc: 0.7727\n",
      "Epoch 214/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7944 - acc: 0.7727\n",
      "Epoch 215/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7893 - acc: 0.7727\n",
      "Epoch 216/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.7848 - acc: 0.7727\n",
      "Epoch 217/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7802 - acc: 0.7727\n",
      "Epoch 218/250\n",
      "22/22 [==============================] - 0s 94us/sample - loss: 0.7745 - acc: 0.7727\n",
      "Epoch 219/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.7691 - acc: 0.7727\n",
      "Epoch 220/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.7637 - acc: 0.7727\n",
      "Epoch 221/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.7586 - acc: 0.7727\n",
      "Epoch 222/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.7533 - acc: 0.7727\n",
      "Epoch 223/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.7474 - acc: 0.7727\n",
      "Epoch 224/250\n",
      "22/22 [==============================] - 0s 80us/sample - loss: 0.7418 - acc: 0.7727\n",
      "Epoch 225/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.7356 - acc: 0.7727\n",
      "Epoch 226/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.7294 - acc: 0.7727\n",
      "Epoch 227/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.7234 - acc: 0.7727\n",
      "Epoch 228/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7168 - acc: 0.7727\n",
      "Epoch 229/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7104 - acc: 0.7727\n",
      "Epoch 230/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.7038 - acc: 0.7727\n",
      "Epoch 231/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.6974 - acc: 0.8182\n",
      "Epoch 232/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.6904 - acc: 0.8182\n",
      "Epoch 233/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.6834 - acc: 0.8182\n",
      "Epoch 234/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.6761 - acc: 0.8182\n",
      "Epoch 235/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.6693 - acc: 0.8182\n",
      "Epoch 236/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.6617 - acc: 0.8182\n",
      "Epoch 237/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.6545 - acc: 0.8182\n",
      "Epoch 238/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.6473 - acc: 0.8182\n",
      "Epoch 239/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.6404 - acc: 0.8182\n",
      "Epoch 240/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.6336 - acc: 0.8182\n",
      "Epoch 241/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.6263 - acc: 0.8182\n",
      "Epoch 242/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.6189 - acc: 0.8182\n",
      "Epoch 243/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.6116 - acc: 0.8182\n",
      "Epoch 244/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.6044 - acc: 0.8182\n",
      "Epoch 245/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.5978 - acc: 0.8182\n",
      "Epoch 246/250\n",
      "22/22 [==============================] - 0s 46us/sample - loss: 0.5901 - acc: 0.8182\n",
      "Epoch 247/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.5840 - acc: 0.8182\n",
      "Epoch 248/250\n",
      "22/22 [==============================] - 0s 91us/sample - loss: 0.5771 - acc: 0.8182\n",
      "Epoch 249/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.5698 - acc: 0.8182\n",
      "Epoch 250/250\n",
      "22/22 [==============================] - 0s 45us/sample - loss: 0.5637 - acc: 0.8636\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(pad_sequences, np.array(y_train), epochs=250, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando a Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    max_len = 50\n",
    "    \n",
    "    print(f'{Fore.RED}Chatbot ativo, envie \"sair\" para finalizar...{Style.RESET_ALL}')\n",
    "    \n",
    "    while True:\n",
    "        print(f'{Fore.GREEN}Usuário: {Style.RESET_ALL}', end=' ')\n",
    "        texto = unidecode(input().lower())\n",
    "        \n",
    "        if texto == 'sair':\n",
    "            break\n",
    "            \n",
    "        result = model.predict(tf.keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([texto]), truncating='post', maxlen=max_len))\n",
    "        label = encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        responses = target.get(label[0], None)\n",
    "        \n",
    "        if responses == None:\n",
    "            print(f'{Fore.RED}Oooops...{Style.RESET_ALL}')\n",
    "            continue\n",
    "        \n",
    "        response = np.random.choice(responses)\n",
    "        \n",
    "        print(f'{Fore.RED}Rose: {Style.RESET_ALL}{response}')\n",
    "        \n",
    "    print(f'{Fore.RED}Chatbot offline...{Style.RESET_ALL}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mChatbot ativo, envie \"sair\" para finalizar...\u001b[0m\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ola\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mRose: \u001b[0mcomo vai?\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tudo bem>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mRose: \u001b[0mSem problemas!\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " tudo bem?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mRose: \u001b[0mSem problemas!\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " preciso agendar um exame\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mRose: \u001b[0mCerto! Preciso de mais informações. Qual exame gostaria de agendar?\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " como você se chama?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mRose: \u001b[0mMeu nome é Rose, sou um chatbot!\n",
      "\u001b[32mUsuário: \u001b[0m "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " sair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mChatbot offline...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
